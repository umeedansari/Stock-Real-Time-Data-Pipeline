# Real-Time Stock Market Data Pipeline | Modern Data Stack

This project demonstrates an end-to-end real-time data pipeline using the Modern Data Stack. We capture live stock market data from an external API, stream it in real time, orchestrate transformations, and deliver analytics-ready insights â€” all in one unified project.

## âš¡ Tech Stack

- **Snowflake** â†’ Cloud Data Warehouse
- **DBT** â†’ SQL-based Transformations
- **Apache Airflow** â†’ Workflow Orchestration
- **Apache Kafka** â†’ Real-time Streaming
- **Python** â†’ Data Fetching & API Integration
- **Docker** â†’ Containerization

## âœ… Key Features

- Fetching live stock market data (not simulated) from Finnhub API
- Real-time streaming pipeline with Kafka
- Orchestrated ETL workflow using Airflow
- Transformations using DBT inside Snowflake
- Scalable cloud warehouse powered by Snowflake



## ğŸš€ Getting Started

1. Clone this repository and set up environment
2. Start Kafka + Airflow services via Docker
3. Run the Python producer to fetch live stock data
4. Data flows into Snowflake â†’ DBT applies transformations
5. Orchestrate everything with Airflow
6. Connect Power BI for visualization

## âš™ï¸ Step-by-Step Implementation

### 1. Kafka Setup

Configured Apache Kafka locally using Docker. Created a `stocks-topic` to handle live stock market events with defined producers (API fetch) and consumers (pipeline ingestion).

### 2. Live Market Data Producer

Developed Python producer script `producer.py` to fetch real-time stock prices from the Finnhub API using an API key. Streams stock data into Kafka in JSON format.


### 3. Kafka Consumer â†’ MinIO

Built Python consumer script `consumer.py` to consume streaming data from Kafka. Stored consumed data into MinIO buckets (S3-compatible storage) organized into folders for raw/bronze layer ingestion.


### 4. Airflow Orchestration

Initialized Apache Airflow in Docker. Created DAG (`minio-to-snowflake.py`) to:
- Load data from MinIO into Snowflake staging tables (Bronze)
- Schedule automated runs every 1 minute

### 5. Snowflake Warehouse Setup

Created Snowflake database, schema, and warehouse. Defined staging tables for Bronze â†’ Silver â†’ Gold layers.

### 6. DBT Transformations

Configured DBT project with Snowflake connection. Models include:

- Bronze models â†’ Raw structured data
- Silver models â†’ Cleaned, validated data
- Gold models â†’ Analytical views (Candlestick, KPI, Tree Map)


## ğŸ“Š Final Deliverables

- âœ… Automated real-time data pipeline
- âœ… Snowflake tables (Bronze â†’ Silver â†’ Gold)
- âœ… Transformed analytics models with DBT
- âœ… Orchestrated DAGs in Airflow

## ğŸ”§ Prerequisites

- Docker and Docker Compose
- Python 3.8+
- Snowflake account
- Finnhub API key

## ğŸ“Œ Architecture
<img width="1212" height="692" alt="image" src="https://github.com/user-attachments/assets/ea2c7e7e-b667-452b-96e1-4ea7c04b76b3" />


